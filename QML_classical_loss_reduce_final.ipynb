{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tm8XleRZkani"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = keras.Input(shape=(784,), name=\"digits\")\n",
        "x1 = layers.Dense(64, activation=\"relu\")(inputs)\n",
        "x2 = layers.Dense(64, activation=\"relu\")(x1)\n",
        "outputs = layers.Dense(10, name=\"predictions\")(x2)\n",
        "model = keras.Model(inputs=inputs, outputs=outputs)"
      ],
      "metadata": {
        "id": "DgOIJGppkhus"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate an optimizer.\n",
        "optimizer = keras.optimizers.SGD(learning_rate=1e-3)\n",
        "# Instantiate a loss function.\n",
        "loss_fn = keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "# Prepare the training dataset.\n",
        "batch_size = 64\n",
        "(x_train, y_train), (x_test, y_test) = keras.datasets.mnist.load_data()\n",
        "x_train = np.reshape(x_train, (-1, 784))\n",
        "x_test = np.reshape(x_test, (-1, 784))\n",
        "\n",
        "# Reserve 10,000 samples for validation.\n",
        "x_val = x_train[-10000:]\n",
        "y_val = y_train[-10000:]\n",
        "x_train = x_train[:-10000]\n",
        "y_train = y_train[:-10000]\n",
        "\n",
        "# Prepare the training dataset.\n",
        "train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "train_dataset = train_dataset.shuffle(buffer_size=1024).batch(batch_size)\n",
        "\n",
        "# Prepare the validation dataset.\n",
        "val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "val_dataset = val_dataset.batch(batch_size)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "85S9049_kjL4",
        "outputId": "c3dd28f4-5f0f-4877-846d-64e10f869ec2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11493376/11490434 [==============================] - 0s 0us/step\n",
            "11501568/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 20\n",
        "for epoch in range(epochs):\n",
        "    print(\"\\nStart of epoch %d\" % (epoch,))\n",
        "\n",
        "    # Iterate over the batches of the dataset.\n",
        "    for step, (x_batch_train, y_batch_train) in enumerate(train_dataset):\n",
        "\n",
        "        # Open a GradientTape to record the operations run\n",
        "        # during the forward pass, which enables auto-differentiation.\n",
        "        with tf.GradientTape() as tape:\n",
        "\n",
        "            # Run the forward pass of the layer.\n",
        "            # The operations that the layer applies\n",
        "            # to its inputs are going to be recorded\n",
        "            # on the GradientTape.\n",
        "            logits = model(x_batch_train, training=True)  # Logits for this minibatch\n",
        "\n",
        "            # Compute the loss value for this minibatch.\n",
        "            loss_value = loss_fn(y_batch_train, logits)\n",
        "\n",
        "        # Use the gradient tape to automatically retrieve\n",
        "        # the gradients of the trainable variables with respect to the loss.\n",
        "        grads = tape.gradient(loss_value, model.trainable_weights)\n",
        "\n",
        "        # Run one step of gradient descent by updating\n",
        "        # the value of the variables to minimize the loss.\n",
        "        optimizer.apply_gradients(zip(grads, model.trainable_weights))\n",
        "\n",
        "        # Log every 200 batches.\n",
        "        if step % 200 == 0:\n",
        "            print(\n",
        "                \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                % (step, float(loss_value))\n",
        "            )\n",
        "            "
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "35hyRLxskkt4",
        "outputId": "ff749210-2d8f-4e45-8a17-f0e8b82eb408"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Start of epoch 0\n",
            "Training loss (for one batch) at step 0: 0.2395\n",
            "Training loss (for one batch) at step 200: 0.3973\n",
            "Training loss (for one batch) at step 400: 0.2544\n",
            "Training loss (for one batch) at step 600: 0.2078\n",
            "\n",
            "Start of epoch 1\n",
            "Training loss (for one batch) at step 0: 0.4666\n",
            "Training loss (for one batch) at step 200: 0.4563\n",
            "Training loss (for one batch) at step 400: 0.2527\n",
            "Training loss (for one batch) at step 600: 0.3200\n",
            "\n",
            "Start of epoch 2\n",
            "Training loss (for one batch) at step 0: 0.2839\n",
            "Training loss (for one batch) at step 200: 0.2129\n",
            "Training loss (for one batch) at step 400: 0.1790\n",
            "Training loss (for one batch) at step 600: 0.3595\n",
            "\n",
            "Start of epoch 3\n",
            "Training loss (for one batch) at step 0: 0.5131\n",
            "Training loss (for one batch) at step 200: 0.3391\n",
            "Training loss (for one batch) at step 400: 0.4190\n",
            "Training loss (for one batch) at step 600: 0.3658\n",
            "\n",
            "Start of epoch 4\n",
            "Training loss (for one batch) at step 0: 0.2544\n",
            "Training loss (for one batch) at step 200: 0.1871\n",
            "Training loss (for one batch) at step 400: 0.1374\n",
            "Training loss (for one batch) at step 600: 0.2654\n",
            "\n",
            "Start of epoch 5\n",
            "Training loss (for one batch) at step 0: 0.0760\n",
            "Training loss (for one batch) at step 200: 0.2591\n",
            "Training loss (for one batch) at step 400: 0.4174\n",
            "Training loss (for one batch) at step 600: 0.2018\n",
            "\n",
            "Start of epoch 6\n",
            "Training loss (for one batch) at step 0: 0.4559\n",
            "Training loss (for one batch) at step 200: 0.1769\n",
            "Training loss (for one batch) at step 400: 0.1357\n",
            "Training loss (for one batch) at step 600: 0.3913\n",
            "\n",
            "Start of epoch 7\n",
            "Training loss (for one batch) at step 0: 0.2872\n",
            "Training loss (for one batch) at step 200: 0.1133\n",
            "Training loss (for one batch) at step 400: 0.2541\n",
            "Training loss (for one batch) at step 600: 0.3680\n",
            "\n",
            "Start of epoch 8\n",
            "Training loss (for one batch) at step 0: 0.2605\n",
            "Training loss (for one batch) at step 200: 0.3307\n",
            "Training loss (for one batch) at step 400: 0.2820\n",
            "Training loss (for one batch) at step 600: 0.1773\n",
            "\n",
            "Start of epoch 9\n",
            "Training loss (for one batch) at step 0: 0.1760\n",
            "Training loss (for one batch) at step 200: 0.2543\n",
            "Training loss (for one batch) at step 400: 0.1910\n",
            "Training loss (for one batch) at step 600: 0.1183\n",
            "\n",
            "Start of epoch 10\n",
            "Training loss (for one batch) at step 0: 0.1889\n",
            "Training loss (for one batch) at step 200: 0.1332\n",
            "Training loss (for one batch) at step 400: 0.2201\n",
            "Training loss (for one batch) at step 600: 0.3010\n",
            "\n",
            "Start of epoch 11\n",
            "Training loss (for one batch) at step 0: 0.0860\n",
            "Training loss (for one batch) at step 200: 0.2052\n",
            "Training loss (for one batch) at step 400: 0.3842\n",
            "Training loss (for one batch) at step 600: 0.0687\n",
            "\n",
            "Start of epoch 12\n",
            "Training loss (for one batch) at step 0: 0.0995\n",
            "Training loss (for one batch) at step 200: 0.3144\n",
            "Training loss (for one batch) at step 400: 0.1045\n",
            "Training loss (for one batch) at step 600: 0.1620\n",
            "\n",
            "Start of epoch 13\n",
            "Training loss (for one batch) at step 0: 0.1145\n",
            "Training loss (for one batch) at step 200: 0.1887\n",
            "Training loss (for one batch) at step 400: 0.1471\n",
            "Training loss (for one batch) at step 600: 0.2398\n",
            "\n",
            "Start of epoch 14\n",
            "Training loss (for one batch) at step 0: 0.2238\n",
            "Training loss (for one batch) at step 200: 0.0759\n",
            "Training loss (for one batch) at step 400: 0.3006\n",
            "Training loss (for one batch) at step 600: 0.2818\n",
            "\n",
            "Start of epoch 15\n",
            "Training loss (for one batch) at step 0: 0.0995\n",
            "Training loss (for one batch) at step 200: 0.0815\n",
            "Training loss (for one batch) at step 400: 0.1735\n",
            "Training loss (for one batch) at step 600: 0.3317\n",
            "\n",
            "Start of epoch 16\n",
            "Training loss (for one batch) at step 0: 0.2824\n",
            "Training loss (for one batch) at step 200: 0.0907\n",
            "Training loss (for one batch) at step 400: 0.2623\n",
            "Training loss (for one batch) at step 600: 0.0886\n",
            "\n",
            "Start of epoch 17\n",
            "Training loss (for one batch) at step 0: 0.0805\n",
            "Training loss (for one batch) at step 200: 0.1942\n",
            "Training loss (for one batch) at step 400: 0.2518\n",
            "Training loss (for one batch) at step 600: 0.1641\n",
            "\n",
            "Start of epoch 18\n",
            "Training loss (for one batch) at step 0: 0.0846\n",
            "Training loss (for one batch) at step 200: 0.1479\n",
            "Training loss (for one batch) at step 400: 0.0710\n",
            "Training loss (for one batch) at step 600: 0.1603\n",
            "\n",
            "Start of epoch 19\n",
            "Training loss (for one batch) at step 0: 0.1791\n",
            "Training loss (for one batch) at step 200: 0.1136\n",
            "Training loss (for one batch) at step 400: 0.0973\n",
            "Training loss (for one batch) at step 600: 0.1369\n"
          ]
        }
      ]
    }
  ]
}